\documentclass[12pt]{article}
\usepackage[autolanguage]{numprint}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage[euler]{textgreek}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
%\usepackage{subfigure}
\usepackage{comment}
\usepackage{caption}
\usepackage{lastpage}
\usepackage[colorlinks,pdfpagelabels,pdfstartview = FitH,bookmarksopen = true,bookmarksnumbered = true,linkcolor = black,plainpages = false,hypertexnames = true,citecolor = black,pagebackref = true,urlcolor = black] {hyperref}
\usepackage{setspace}
\usepackage{silence}
\WarningFilter{latex}{Text page}
\usepackage{parskip}
\newcommand{\cms}{~cm\textsuperscript{-2}s\textsuperscript{-1} }

\graphicspath{{figures/}}   
% \renewcommand{\figurename}{Fig.}
\addto\captionsfrench{\renewcommand{\figurename}{Fig.}}

\setlength{\parskip}{4mm plus2mm minus2mm}
\setlength{\parindent}{1cm}

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here


\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Universit\'e Claude Bernard Lyon I}\\[1.5cm] % Name of your university/college
\textsc{\Large Master Data Science}\\[0.5cm] % Major heading such as course name
\textsc{\large Projet Data Mining}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries D\'etection d'\'ev\'enements sur Twitter}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Auteurs:}\\
Gregory \textsc{Howard} \textit{11207726} \\ 
Marine \textsc{Ruiz} \textit{11208141} \\ 
Jules \textsc{Sauvinet} \textit{11412086}
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Professeur:} \\
Marc \textsc{Plantevit} % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[1cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[height=3cm]{twitter}\vspace{2cm}
\includegraphics[height=3cm]{ucbl}\\
 % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

\begin{abstract}
La d\'etection d'\'ev\`enements est un des sujets de recherche les plus importants dans l'analyse des r\'eseaux sociaux.
Les flux de donn\'ees provenant des plates-formes de ces r\'eseaux contiennent, la plupart du temps, beaucoup d'informations. On peut traiter et analyser ces informations dans le but de faire de la d\'etection d'\'ev\`enement(s). 
\newline
N\'eanmoins, une grande partie des donn\'ees \`a traiter est bruit\'ee, notamment par des comptes Twitter entretenus par des robots. Ces comptes postent des tweets en masse avec une fr\'equence de publication constante, ce qui nous emp\^echent de donner une bonne description d\' un \'ev\'enement ou alors on peut cr\'eer de faux \'ev\`enements. 
\newline
Voila pourquoi il est important de comprendre comment att\'enuer l'influence du bruit pour faire de la d\'etection \'ev\`enements. Notre objectif a \'et\'e de d\'etecter des \'ev\`enements \`a partir de tweets en exploitant des informations spatio-temporelles et textuelles en essayant d'\'ecarter les tweets de robots. 
\end{abstract}

%\begin{spacing}{1.15}
\pdfbookmark[1]{Contents}{toc}
\small{
\tableofcontents 
}

\newpage
% \listoffigures
% \newpage
% \listoftables 
%\end{spacing}

\section{Introduction}

Notre objectif a \'et\'e de d\'etecter des \'ev\`enements \`a partir de tweets en exploitant des informations spatio-temporelles et textuelles. Notre approche utilise la technique de d\'etection d'\'ev\'enements multi-\'echelles dans les r\'eseaux sociaux introduite dans l'article de recherche \cite{Multievents} et d\'evelopp\'e par \textsc{Ahmed Anes Bendimerad} et \textsc{Aimene Belfodil}. Nous avons adapt\'e cette version \`a nos donn\'ees \`a laquelle nous avons ajout\'e des m\'ecanismes permettant de filtrer les tweets envoy\'es par des robots en amont du clustering permettant la d\'etection d'\'ev\`enements. Les comptes automatisés, appel\'es robots, abusent des r\'eseaux sociaux en affichant du contenu non-\'ethique, soutenant des activit\'es parrain\'ees ou utilisant leur compte pour de la vente. Ces robots ne font qu'apporter une couche de bruits puisqu'il ne d\'evrivent pas d'\'ev\`enement, faussant aisni notre clustering.

Nous avons \'egalement d\'evelopp\'e une approche de d\'etection spatio-temporelle d'\'ev\`enements avec l'algorithme de clustering DBScan puis nous avons compar\'e les r\'esultats des deux types de clustering.

Nous appelons \'ev\`evenement (au sens \'ev\`evenement que nous souhaitons d\'etecter) un ph\'enom\`ene physique survenant en un point et pendant une dur\'ee bien d\'etermin\'e. Un \'ev\'enement est int\'eressant s'il suscite un nombre suffisant de tweets. Ces tweets peuvent se situer sur le lieu de l'\'ev\`enement ou bien ailleurs, notamment si celui-ci est retransmis par les m\'edias. Les tweets sur l'\'ev\`enement peuvent se produire avant, pendant et apr\`es l'\'ev\'enement. Ainsi, la d\'etection d'\'ev\`enement doit r\'eussir \`a int\'egrer ces dispersions pour localiser dans les temps et l'espace les \'ev\`enements et c'est ce qui la rend complexe.

\section{L'approche globale}

Pour notre premier algorithme nous appliquons, sur chaque ensemble journalier de tweets l'algorithme de d\'etection d'\'ev\`enement multi-\'echelle décrite ici \autoref{sec:multiscale}.

Pour notre second algorithme, nous appliquons le premier algorithme mais nous supprimons en amont les comptes robots
que nous sommes susceptibles d'avoir dans nos donn\'ees \`a l'aide d'un worlflow Knime bas\'e sur le nombre de tweets post\'es par les utilisateurs puis par autre filtre bas\'e sur la r\'ecurrence de post des tweets. 

De plus, chacun de nos \'ev\'enements est d\'ecrit par des hashtags pertinents. Ces hashtags sont d\'efinis comme \'etant les plus r\'ecurrents dans les tweets contenus dans l'\'ev\'enement. Cependant, certains hashtags sont pr\'esents dans la description de
beaucoup d'autres \'ev\'enements, ce qui les rend moins pertinents. Nous voulons donc les d\'etecter et ne pas les prendre en compte lors du clustering afin d'avoir une vraie description de l'\'ev\'enement.

\newpage

\section{Description de nos donn\'ees}
\label{sec:desc_donnees}

Nous avons effectu\'e notre d\'etection d'\'ev\`evenents \`a partir d'un fichier de 1,7 Go comportant \nombre{10982005} tweets  r\'ecup\'er\'es sur Twitter. Ces tweets ont \'et\'e post\'es dans l'agglom\'eration newyorkaise ou par des utilisateurs newyorkais du \textit{21 juillet 2015} au \textit{16 novembre 2015}. Chaque tweet contient les informations suivantes :

\begin{itemize}
	\item un identifiant
	\item la date et l'heure \`a laquelle le tweet a \'et\'e post\'e
	\item la position de la personne quand le tweet a \'et\'e post\'e (latitude et longitude)
	\item l'identifiant du lieu g\'eographique o\`u a \'et\'e post\'e le tweet
	\item l'ensemble des r\'ef\'erences et hashtags du tweet
	\item l'identifiant de la personne qui a post\'e le tweet
	\item le pseudo de la personne qui a post\'e le tweet
	\item le nom complet de la personne qui a post\'e le tweet
	\item l'identifiant du lieu de cr\'eation du compte de la personne qui a post\'e le tweet
	\item l'information de la certification ou non du compte de l'utilisateur ayant post\'e le tweet
	\item le nombre de personnes qui suivent la personne qui a post\'e le tweet
	\item le nombre d'amis de la personne qui a post\'e le tweet
	\item le nombre de tweet d\'ej\`a post\'e par la personne qui a post\'e le tweet
\end{itemize}

Pour le d\'etection d'\'ev\`enements, nous nous sommes servi de l'identifiant du tweet, du nom de la personne qui a post\'e le
tweet, des hashtags, de la date et de la position d'envoi du tweet.

\section{Le pr\'e-traitement des donn\'ees}

Nous avons utilis\'e Knime pour effectuer un premier filtrage des robots. Pour cela, nous classons dans l'ordre d\'ecroissant les utilisateurs en fonction du nombre de tweets qu'ils ont post\'es. Puis, nous supprimons les 0.5\% premiers utilisateurs \`{a} partir de ce classement. Ce pr\'e-traitement nous permet d'homog\'en\'eiser nos donn\'ees mais aussi d'augmenter de fa\c{c}on \'evidente la pertinence des \'ev\`enements d\'etect\'es. 

Nous avons bas\'e ce choix de filtrage sur les deux crit\`eres suivants. En effet, d'une part, il y a une probabilit\'e importante qu'un utilisateur qui poste en tr\`es grande quantit\'e des tweets soit un robot. D'autre part, nous nous sommes dit qu'un utilisateur r\'eel ayant une activit\'e sur Twitter bien sup\'erieure \`{a} celle des autres utilisateurs, avait en moyenne moins de contenu descriptif d'\'ev\`evenement dans ses tweets et plus de "tweets bruits", c'est \`{a} dire des tweets nous donnant aucune information sur le d\'eroulement d'un quelconque \'ev\`evenement (e.g "\#lol", ou encore "\#oklm avec mon refr\'e"). Nous avons ensuite confirm\'e cette intuition empiriquement en observant notre jeu de donn\'ees.  

Cette technique \'etant tr\`es approximative, nous ne l'utilisons que pour filtrer les utilisateurs qui publient, vraiment de fa\c{c}on \'evidente, plus que tous les autres utilisateurs. En fait, puisque nous faisons de la d\'etection in fine et non de la suppression de compte par exemple, nous pouvons nous permettre d'avoir des faux n\'egatifs (utilisateurs consid\'er\'e comme robot alors qu'ils ne le sont pas). 

La dimension g\'eographique \'etant cruciale dans notre m\'ethode de clustering, nous avons \'egalement filtr\'e les tweets ne poss\'edant pas de position g\'eographique dans le module Knime. Nous avons donc donn\'e en entr\'ee \`{a} Knime notre fichier de 1,7 Go contenant \nombre{10982005} tweets et nous avons obtenu, \`{a} la fin du traitement, un fichier de 250 Mo contenant \nombre{1600000} tweets. Nous avons joint le workflow Knime robotFilter.knwf permettant de faire ce filtrage \`a l'archive de rendu du projet.

De plus, notre jeu de donn\'ees de tweets s'\'etale sur $103$ jours. Puisque nous voulons faire de la d\'etection d'\'ev\`enements journaliers et localis\'es et pour optimiser les performances, nous avons d\'ecoup\'e notre jeu de donn\'ee initial en 103 sous-ensembles journaliers. Chaque sous-ensemble de tweets contient plus de \nombre{15000} tweets.

\vfill
\newpage

\section{La d\'etection d'\'ev\`enements multi-\'chelle}
\label{sec:multiscale}

La technique de d\'etection d'\'evenements n'est autre que celle \'evoqu\'ee dans l'article \cite{Multievents}.

L'algorithme calcule la matrice de similarit\'e et construit les clusters.

MOI
\newline 
MOI 
\newline
MOI

\subsection{Adaptation de l'algorithme de d\'etection multi-\'echelle et int\'egration du filtrage des robots}

On enl\`eve ceux qui sont r\'egi par une loi G\'eom\'etrique (ici ou dans pretraitement des donnees)

\newpage

\section{Description des \'el\'ements utiles \`a la d\'etection d'\'ev\'enements}

\subsection{Le tweet}
Un tweet est d\'ecrit par : 

\begin{itemize}
\item son identifiant
\item le nom du Tweetos
\item les hashtags
\item la date et l'heure \`a laquelle le Tweetos a tweet\'e
\item sa position au format latitude/longitude au moment du tweet
\item La date
\end{itemize}

Nous appliquons donc notre algorithme de d\'etection d'\'evenements sur chacun de nos sous-ensemble de tweets journaliers.

\subsection{La matrice de similarit\'e}
La matrice de similarit\'e est une matrice triangulaire sup\'erieure de taille (nombre de
tweets) * (nombre de tweets).
\newline
Rappeler ce qui a ete dit dans la partie 5
\begin{itemize}
	\item D\'ecrire construction
	\item reciter l'article en parlant rapidement de la compression de Haar
\end{itemize}

\subsection{Le clustering}
Le clustering se fait \`a partir de la matrice de similarit\'e qui elle m\^eme a \'et\'e obtenue en traitant 
un sous-ensemble de tweets journaliers.
\newline
Rappeler ce qui a ete dit dans la partie 5
\begin{itemize}
	\item Redonner l'algo
	\item Citer le jar et expliquer rapidement ce qu'on maximise / minimise dans l'algorithme
\end{itemize}

\subsection{L'\'ev\'enement}
On d\'ecrit un \'ev\'enement par :
\begin{itemize}
\item une heure de d\'ebut et une heure de fin qui permettent de calculer une dur\'ee
\item une position centrale (longitude, latitude) calcul\'ee \`a partir de la moyenne de toutes
les positions (moyenne des longitudes, moyenne des longitudes)
\item le nombre de personnes diff\'erentes ayant post\'e des tweets
\item la liste des 20 hashtags les plus importants permettant de le d\'ecrire
\end{itemize}
\paragraph{}
Nous r\'ecup\'erons tous les clusters obtenus lors du clustering et ne gardons que les clusters pertinents. 
Un cluster est pertinent si le nombre de personnes ayant tweet\'e est assez important, s'il y a assez de tweets et 
si la proportion de tweets par personne n'est pas \'exag\'er\'ee. Ce cluster devient alors un \'ev\'enement.

\newpage

\section{Les options sur le clustering}

\subsection{L'\'elasticit\'e}

On suppose qu'un hashtag est fr\'equent pour un ensemble de tweets donn\'e s'il apparait
plus de 20 fois.

Ce nombre est statique, ce qui ne permettrait pas de d\'etecter de petits \'ev\'enements dans
une journ\'ee o\`u il y a eu peu de tweets. A l'inverse, on d\'etecte beaucoup d'\'ev\'enements sur
une journ\'ee o\`u il y a eu beaucoup de tweets.
Cela peut \^etre int\'eressant, mais il est parfois plus judicieux de d'adapter au nombre de
tweets post\'es dans une m\^eme journ\'ee. Ainsi, on d\'efinit un hashtag fr\'equent s'il apparait
dans plus de 20\% des tweets.

Pour coder cette option nous avons d\'efinit un bool\'een, que l'on met \`a Vrai pour activer
l'\'elasticit\'e et \`a Faux pour ne pas l'activer.

Les valeurs 20 et 20\% peuvent \^etre modif\'ees gr\^ace aux variables globales d\'efinies au d\'ebut du
module d'\'ex\'ecution Python.

\subsection{La g\'eolocalisation}


\subsection{La loi de Poisson g\'eographique}


\section{La d\'etection d\'ev\`enement avec DBScan}


\section{R\'esultats}



\section{Conclusion}
Les combinaisons int\'eressantes
\newline
Quand, comment


\section*{Acknowledgments}
Nos remerciements vont \`a Marc Plantevit pour l'enseignement de son cours "Data Mining" \`a l'Universit\'e Claude Bernard Lyon I et son accompagnement durant ce projet.



\bibliographystyle{abbrv}
\bibliography{dataminingbib}

\appendix

\end{document}














